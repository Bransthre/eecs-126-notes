\chapter{Bayes Rule, Independence, Discrete Random Variables}

\section{Conditional Probability}
Conditional probabilty is mostly a definition-based mechanism, built along the definition below:
\begin{ln-define}{Conditional PRobability}{}
    If $B \in \mathcal{F}, \P(B) > 0$, then for some other $A \in \mathcal{F}$, then the probability that $A$ occurs given $B$ occurs is:
    \[\P(A | B) := \frac{\P(A \cap B)}{\P(B)}\]
    And mathematically formally, we may phrase it as a function of some event $A$ that restricts a probability model $(\Sigma, \mathcal{F}, \P)$ into samples in $B$, upon which we acquire a new probability model $(B, \mathcal{F}|_B, \P(\cdot | B))$.
    \tcblower
    \textbf{Example} Let there be $A_1, \dots, A_n \in \mathcal{F}$ where $\forall i \in \{1, \dots, n\}, \P(A_i) > 0$, and these $A_i$ are partitions of $\Omega$. \\
    Then, we see that the total probability rule, mathematically stating
    \[\P(B) = \sum_i \P(B \cap A_i) = \sum_i \P(B | A_i)\P(A_i)\]
    is in fact a sum of likelihood of $B$ under each possible partition $A_i$.
\end{ln-define}
And another rule tightly knitted to the notion of conditional probability would be Bayes Rule:
\begin{ln-define}{Bayes Rule}{}
    In a descriptive model, conditional probabilities $\P(B|A)$ also tells the probability of some observation $B$ based on an occuring state $A$. \\
    Therefore conditional probabilities provide us the likelihood of observation based on state. What about the inverse? What mathematical work is there to tell us the probability of a state provided some observation? \\
    This is where we may employ \textbf{Bayes Rule}: 
    \[\P(A|B) = \frac{\P(B \cap A)}{\P(B)} = \frac{\P(B|A) \P(A)}{\P(B)}\]
    \tcblower
    \textbf{Example} Say, that $85\%$ of patients are healthy, and $15\%$ are unhealthy. \\
    Then, say each patient exposes honestly whether they are exposed under some unhealthy material $M$, where:
    \[
        \begin{cases}
            40\% \text{of healthy patients are exposed to } M \\
            90\% \text{of unhealthy patients are exposed to } M
        \end{cases}
    \]
    To assess the gap of healthiness, we may attempt to quantify a statistics of:
    \[k = \frac{\P(Unhealthy | Exposed)}{\P(Unhealthy | Unexposed)}\]
    and Bayes Rule provides us that this statistic, quantifying how much likely is a patient unhealthy upon exposure to material $M$, may be computed via Bayes' Rule as:
    \[
        k = \frac{\P(Unhealthy \cap Exposed) \P(Healthy)}{\P(Unhealthy \cap Unexposed) \P(Unhealthy)} \approx 9.9
    \]
\end{ln-define}

Let us see another example for fun:
\begin{ln-explain}{Conditional Probability and Die}{}
    Die, as in dice rolls! (Because die is a form of word dice). \\
    Say we roll 2 fair, six-sided dices and the sum is $10$. What is the probability that the first roll resulted in a $4$?
    \tcblower
    Let $B$ be the observation that the sum of rolls is $10$, and $A$ is the state that the first roll is $4$. \\
    Then, 
    \begin{align*}
        \P(A | B) &= \frac{\P(A \cap B)}{\P(B)} \\
        &= \frac{\frac{1}{36}}{\frac{3}{36}} = \frac{1}{3}
    \end{align*}
\end{ln-explain}

\subsection{Event Decomposition}
Conditioning also allows us to decompose events into smaller events, making probability computation easier for complicated sample spaces.
\begin{ln-explain}{Event Decomposition}{}
    Consider event $A_1 \cap \cdots \cap A_n$, then such event's probability can be computed simply as:
    \begin{align*}
        \P\bigg(\bigcap_{i = 1}^n A_i\bigg)
        &= \P(A_1) \P\bigg(\bigcap_{i = 2}^n A_i \bigg| A_1\bigg) \\
        &= \P(A_1) \P(A_2 | A_1) \P(A_3 | \P(A_1 \cap A_2) \cdots \P(A_n | \P\bigg(\bigcap_{i = 1}^{n - 1} A_i \bigg| A_1\bigg)))
    \end{align*}
\end{ln-explain}
\begin{ln-practice}{Solving the Birthday Paradox}{}
    Let us suppose the event
    \[A_i = \{\text{person $i$ does not share birthday with anyone}\}\]
    We may compute the conditional probability of this upon other $n - 1$ people not sharing birthday as:
    \[\P \bigg(A_i \bigg| \bigcap_{j=1}^{i - 1} A_j \bigg) = \frac{365 - (i - 1)}{365}\]
    Then, computing for the probability that everyone doesn't share a birthday can be:
    \[
        \P \bigg( \bigcap_{i = 1}^n A_i \bigg) = \prod_{i = 1}^n \P \bigg( A_i \bigg| \bigcap_{j = 1}^{i - 1} A_j \bigg) = \prod_{i = 1}^n \bigg(1 - \frac{i - 1}{365} \bigg)
    \]
    Remarkably, having $n = 23$ gives you a probability of $0.5$.
\end{ln-practice}

\subsection{Monty Hall, the TV Show Goat-Car Scenario (Pasted from my CS70 Notes)}
You have probably visited this scenario on mathematics-related YouTube videos, or movies like 21. \\
If not, don't worry, here is a description of the Monty Hall Problem:
\begin{quote}
    You are now on a TV Show, where three gates are presented in front of you. There are also two goats and one car, each arranged to be behind one door. \\
    You are invited to select one initial door, behind which will be your prize. \\
    Then, the game show host, who knows which gate hides the car, will reveal the other gate that is neither the gate you choose nor the gate the car is behind. \\
    The game show host now asks, ``Will you switch the gate?'' Do you switch the gate?
\end{quote}
Let us mark this as a discrete probability problem. Characterize each sample point as a triplet:
\[(\text{Door of prize}, \text{Initial chosen door}, \text{Door opened by host})\]
The only constraint for the triplet would then be $i \neq k$ (since the game show host would not open a door with car behind it).
In that case, let us further categorize the possible sample points:
\[
    \begin{cases}
        (i, j, k), &\text{Type A: Each of the doors are distinct, the initial pick was not a car} \\
        (i, i, k), &\text{Type B: The initial pick was the car}
    \end{cases}
\]
Here:
\begin{bindenum}
    \item[] For Type A sample points, there are $6$ possible such situations, and the probability of encountering such a sample point is $\frac{1}{3} \times \frac{1}{3} \times \frac{1}{1}$.
    \item[] For Type B sample points, meanwhile, there are also $6$ possible such situations, and the probability of encountering such a sample point is $\frac{1}{3} \times \frac{1}{3} \times \frac{1}{2}$.
\end{bindenum}
Such a sample space does satisfy the non-negative and total-one properties, and therefore is applicable for use. \\
In this case, the probability of winning by switching a door would be the probability at which the sample point is type A, such that switching leads to $i = j$. The resulting probability of winning by switching is then $\frac{6}{9} = \frac{2}{3}$.

Alternatively, let us consider a total summary of Monty Hall contexts:
\begin{ln-practice}{Solve for the General Monty Hall Solution}{}
    For $k$ revealed doors and $c$ cars (each behind a door), the probability that we have picked the current prize door, resulting in a futile switch, is $\frac{c}{n}$. \\
    The probability that a switch works can be computed as:
    \[
        \frac{c \cdot (c - 1) + (n - c) \cdot c}{n \cdot (n - 1 - k)} = \frac{c \cdot (n - 1)}{n \cdot (n - 1 - k)}
    \]
    By counting all situations where a switch works either the initial choice was correct or not, and offered that the second round of choice only has $n - 1 - k$ possible options. \\

    Here, we see that setting $c = 1$ and $k = 1$ reflects the original Monty Hall choice problem, where the probability of winning by switch is as well computed to be $\frac{2}{3}$.
\end{ln-practice}

\section{Independence}
The definition of such property is as follows:
\begin{ln-define}{Independence}{}
    Events $A, B$ are independent if $\P(A \cap B) = \P(A) \P(B)$. \\
    As a reminder, disjoint probabilities might still be dependent. \\

    Note, also, that in a special case where $\P(A) > 0$,
    \[A, B \text{ are independent} \iff \P(B | A) = \P(B)\]
\end{ln-define}
In general, the collection $A_1, \dots, A_n \in \mathcal{F}$ are independent events if
\[\P\bigg( \bigcap_{i \in S} A_i = \prod_{i \in S} \P(A_i) \bigg)\]
for all finite sets of indices $S$.

Now, let us discuss the consequence of independence.
\begin{ln-explain}{Independence of Collection and Complements}{}
    If $A_i, \dots \in \mathcal{F}$ are independent, then for $B_i = A_i \text{ or } A_i^C$, the collection $B_i$ are also independent.
    \tcblower
    \textbf{Proof}. First we may recognize that we can decompose the following intersection into,
    \[
        \bigcap_{i = 2}^n A_i = \bigg( \bigcap_{i = 1}^n A_i \bigg) \sqcup \bigg( A_1^C \cap \bigcap_{i = 2}^n A_i \bigg)
    \]
    Where upon we recognize:
    \begin{align*}
        \prod_{i = 2}^n \P(A_i)
        &= \P(\bigcap_{i = 2}^n A_i) \\
        &= \P \bigg( \bigg( \bigcap_{i = 1}^n A_i \bigg) \sqcup \bigg( A_1^C \cap \bigcap_{i = 2}^n A_i \bigg) \bigg) \\
        &\underset{A3}{=} \prod_{i = 1}^n \P(A_i) + \P \bigg( A_1^C \cap \bigcap_{i = 2}^n A_i \bigg)
    \end{align*}
    And upon the total probability rule we recognize that $A_1^C, A_2, \dots, A_n$ are independent because,
    \[(1 - \P(A_1)) \prod_{i = 2}^n \P(A_i) = \P \bigg( A_1^C \cap \bigcap_{i = 2}^n A_i \bigg)\]
\end{ln-explain}

\subsection{Conditional Independence}
Yes. For anything there is a conditional version:
\begin{ln-define}{Conditional Independence}{}
    If $A, B, C \in \mathcal{F}$ are such that $\P(C) > 0$, and $\P(A \cap B | C) = \P(A | C) \P(B | C)$, then $A, B$ are said to be conditionally indepenedent given $C$.
    \tcblower
    \textbf{Example.} We may pick a coin at random, with said probability $\frac{1}{2}$, and we flip twice. \\
    Say, that the event $H_i$ resembles the event where the $i^{th}$ flip of coin is a head. \\

    We can see that $H_1$ and $H_2$ are not independent. \\
    Here, let us say that the coins have bias $p$ and $q$ instead:
    \[
        \P(H_i) = \frac{p + q}{2}
    \]
    Then,
    \[
        \P(H_1 \cap H_2) = \frac{p^2 + q^2}{2} \neq \P(H_1) \P(H_2) = {\bigg( \frac{p + q}{2} \bigg)}^2
    \]
    However, let $C$ be the event that we picked the coin with bias $p$, then we see $H_1, H_2$ are conditionally idependent given $C$.
\end{ln-define}
